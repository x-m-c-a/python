import jieba
from collections import Counter
from torch.utils.data import Dataset, DataLoader
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.nn.utils.rnn import pad_sequence
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# 数据定义
data = {
    'text': [
        "我爱这部电影", "这是一部伟大的电影", "我讨厌这部电影", "这部电影很糟糕",
        "演技非常出色", "剧情很无聊", "导演非常棒", "故事很感人",
        "音乐很动听", "画面很美", "演员表现很好", "剧情很复杂",
        "情节很紧凑", "特效很震撼", "角色塑造得很好", "故事很平淡",
        "场景设计很棒", "剧情很有趣", "演员演技很差", "剧情拖沓",
        "特效一般", "故事很深刻", "音乐很好听", "画面很清晰",
        "演员表现一般", "剧情很曲折", "情节很吸引人", "特效很逼真",
        "角色很有魅力", "故事很精彩", "场景设计一般", "剧情很平淡",
        "演员表现优秀", "剧情很紧凑", "情节很生动", "特效很出色",
        "角色塑造得很差", "故事很一般", "场景设计出色", "剧情很有趣",
        "演员表现糟糕", "剧情很精彩", "情节很吸引人", "特效很震撼",
        "角色很有魅力", "故事很平淡", "场景设计一般", "剧情很平淡",
        "演员表现优秀", "剧情很紧凑", "情节很生动", "特效很出色"
    ],
    'label': [1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0]
}

train_x = data['text']
train_y = data['label']

# 分词
train_x = [list(jieba.cut(x)) for x in train_x]

# 停用词过滤
with open(r'C:\Users\LENOVO\.jupyter\hit_stopwords.txt', 'r', encoding='UTF8') as f:
    stop_words = [word.strip() for word in f.readlines()]

def drop_stopword(datas):
    for data in datas:
        for word in data[:]:
            if word in stop_words:
                data.remove(word)
    return datas

train_x = drop_stopword(train_x)

# 构建词汇表
def build_vocab(texts, min_count=1):
    all_words = [word for doc in texts for word in doc]
    word_counts = Counter(all_words)
    vocab = [word for word, count in word_counts.items() if count >= min_count]
    word2index = {word: idx for idx, word in enumerate(vocab)}
    index2word = {idx: word for word, idx in word2index.items()}
    return vocab, word2index, index2word

vocab, word2index, index2word = build_vocab(train_x)

# 数据加载器
class TextDataset(Dataset):
    def __init__(self, texts, labels, word2index, max_len=50):
        self.texts = texts
        self.labels = labels
        self.word2index = word2index
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        indices = [self.word2index[word] if word in self.word2index else 0 for word in text][:self.max_len]
        # 填充或截断
        if len(indices) < self.max_len:
            indices += [0] * (self.max_len - len(indices))
        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)

# 划分训练集和验证集
train_ratio = 0.8
train_size = int(len(train_x) * train_ratio)
train_dataset = TextDataset(train_x[:train_size], train_y[:train_size], word2index)
val_dataset = TextDataset(train_x[train_size:], train_y[train_size:], word2index)

# 自定义的 collate 函数
def custom_collate_fn(batch):
    inputs, labels = zip(*batch)
    # 将输入序列填充到相同的长度
    padded_inputs = pad_sequence(inputs, batch_first=True)
    # 将标签转换为张量
    labels = torch.tensor(labels, dtype=torch.long)
    return padded_inputs, labels

# 创建 DataLoader
train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=custom_collate_fn)
val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=custom_collate_fn)

# 模型定义
class TextClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout=0.5):
        super(TextClassifier, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.fc1 = nn.Linear(embedding_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        embedded = self.embedding(x)
        pooled = F.max_pool1d(embedded.transpose(1, 2), embedded.size(1)).squeeze(2)
        dropped = self.dropout(pooled)
        hidden = F.relu(self.fc1(dropped))
        output = self.fc2(hidden)
        return output

# 参数设置
vocab_size = len(vocab)
embedding_dim = 100
hidden_dim = 128
output_dim = 2  # 正面和负面两类

model = TextClassifier(vocab_size, embedding_dim, hidden_dim, output_dim)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 训练模型
num_epochs = 100
best_val_loss = float('inf')
best_model_path = 'best_model.pth'

def evaluate_model(model, dataloader):
    model.eval()
    all_preds = []
    all_labels = []
    total_loss = 0
    with torch.no_grad():
        for batch in dataloader:
            inputs, labels = batch
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            total_loss += loss.item()
            _, preds = torch.max(outputs.data, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    accuracy = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, average='binary')
    recall = recall_score(all_labels, all_preds, average='binary')
    f1 = f1_score(all_labels, all_preds, average='binary')
    return total_loss / len(dataloader), accuracy, precision, recall, f1

for epoch in range(num_epochs):
    model.train()
    total_train_loss = 0
    for batch in train_loader:
        inputs, labels = batch
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_train_loss += loss.item()

    # 验证模型
    val_loss, val_accuracy, val_precision, val_recall, val_f1 = evaluate_model(model, val_loader)

    # 打印训练和验证结果
    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_train_loss / len(train_loader)}, '
              f'Val Loss: {val_loss}, Val Accuracy: {val_accuracy}, Val Precision: {val_precision}, '
              f'Val Recall: {val_recall}, Val F1 Score: {val_f1}')

    # 保存最佳模型
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), best_model_path)
        print(f'Saved best model with validation loss: {best_val_loss}')

# 加载最佳模型
model.load_state_dict(torch.load(best_model_path, weights_only=True))

# 添加情感分析功能
def preprocess_input(input_text, word2index, max_len=50):
    # 分词
    words = list(jieba.cut(input_text))
    # 停用词过滤
    words = [word for word in words if word not in stop_words]
    # 转换为索引
    indices = [word2index[word] if word in word2index else 0 for word in words][:max_len]
    # 填充或截断
    if len(indices) < max_len:
        indices += [0] * (max_len - len(indices))
    return torch.tensor(indices, dtype=torch.long).unsqueeze(0)

def predict_sentiment(input_text, model, word2index):
    model.eval()
    input_tensor = preprocess_input(input_text, word2index)
    with torch.no_grad():
        output = model(input_tensor)
        _, predicted = torch.max(output.data, 1)
    sentiment = "正面" if predicted.item() == 1 else "负面"
    return sentiment

# 测试情感分析功能
input_text = "这部电影真的很好看"
sentiment = predict_sentiment(input_text, model, word2index)
print(f'输入文本: "{input_text}" 的情感是: {sentiment}')

# 最终评估
final_val_loss,final_val_accuracy
print(f'Final Validation Loss: {final_val_loss}')
print(f'Final Validation Accuracy: {final_val_accuracy}')
